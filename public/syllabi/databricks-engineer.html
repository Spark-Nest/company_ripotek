<article>
  <h1>Databricks Engineer Program (24 Weeks)</h1>
  <p><strong>Duration:</strong> 24 Weeks &nbsp; | &nbsp; <strong>Investment:</strong> CAD $3,999 &nbsp; | &nbsp; <strong>Level:</strong> Intermediate to Advanced &nbsp; | &nbsp; <strong>Schedule:</strong> Mon/Wed/Sat 6–9pm MT</p>

  <h2>Program Overview</h2>
  <p>Learn Spark, Delta Lake, and modern lakehouse engineering on the Databricks platform. Master PySpark, medallion architecture, and production-grade orchestration. Aligned to <em>Databricks Certified Data Engineer Associate</em>.</p>

  <h3>Learning Outcomes</h3>
  <ul>
    <li>Engineer scalable data pipelines with PySpark and SQL</li>
    <li>Implement Delta Lake with ACID transactions and time travel</li>
    <li>Design Bronze/Silver/Gold layers for analytical performance</li>
    <li>Optimize Spark jobs (partitions, caching, joins, shuffles)</li>
    <li>Govern data with Unity Catalog; productionize with Workflows</li>
    <li>Track and deploy models with MLflow</li>
  </ul>

  <h2>Curriculum</h2>
  <h3>Phase 1 (Weeks 1–6): Spark &amp; PySpark Fundamentals</h3>
  <ul>
    <li>Databricks workspace, clusters, DBU model, auto-termination</li>
    <li>Spark architecture (driver/executors) and lazy evaluation</li>
    <li>RDDs vs DataFrames; schema, types, I/O (CSV/JSON/Parquet/Delta)</li>
    <li>Transformations and actions; SQL in Spark</li>
    <li>Joins, aggregations, window functions in PySpark</li>
  </ul>
  <p><strong>Labs:</strong> Create cluster &amp; notebook; load Parquet and Delta; implement joins and window analytics; benchmark transformations.</p>

  <h3>Phase 2 (Weeks 7–12): Delta Lake &amp; Medallion</h3>
  <ul>
    <li>Delta tables: schema enforcement/evolution, CDC (change data feed)</li>
    <li>CRUD, MERGE; OPTIMIZE and VACUUM; Z-ORDER for performance</li>
    <li>Medallion design: raw (Bronze), curated (Silver), aggregated (Gold)</li>
    <li>Auto Loader for incremental ingestion; quality checks and quarantine</li>
    <li>SCD Type‑2 with Delta and surrogate keys</li>
  </ul>
  <p><strong>Labs:</strong> Build Bronze → Silver ingestion with Auto Loader; implement SCD‑2 MERGE; apply OPTIMIZE/VACUUM; Z‑ORDER by query columns.</p>

  <h3>Phase 3 (Weeks 13–18): Advanced Spark &amp; Performance</h3>
  <ul>
    <li>Execution model: jobs, stages, tasks; shuffle mechanics</li>
    <li>Partitioning strategies; caching and persistence</li>
    <li>Broadcast joins; UDFs and pandas UDFs</li>
    <li>Cost/performance tuning and troubleshooting with Spark UI</li>
  </ul>
  <p><strong>Labs:</strong> Diagnose shuffle hotspots; refactor to broadcast joins; compare caching strategies; profile jobs in Spark UI.</p>

  <h3>Phase 4 (Weeks 19–24): Production Engineering &amp; MLOps</h3>
  <ul>
    <li>Databricks Workflows (Jobs), DAGs, parameters, alerts</li>
    <li>Unity Catalog for governance, permissions, and auditing</li>
    <li>Event-driven ingestion with Auto Loader; streaming patterns</li>
    <li>Azure integrations (ADF, Event Hubs, Synapse) in the lakehouse</li>
    <li>MLflow: experiment tracking, model registry, deployment</li>
  </ul>
  <p><strong>Labs:</strong> Orchestrate Bronze→Silver→Gold as a multi‑task job; add alerts; register a model in MLflow and deploy a simple batch scoring task.</p>

  <h2>Assessment &amp; Outcomes</h2>
  <ul>
    <li><strong>Capstone:</strong> Build an end-to-end lakehouse implementation</li>
    <li><strong>Certification:</strong> Databricks Data Engineer Associate (80% pass rate)</li>
    <li><strong>Placement:</strong> 85% within 90 days; average salary $105K–$135K</li>
  </ul>

  <h2>Policies</h2>
  <ul>
    <li><strong>Attendance:</strong> 80% minimum; recordings available; request excused absences 24h in advance when possible</li>
    <li><strong>Refunds:</strong> Full (minus $200) before Week 2; 50% during Weeks 2–4; none after Week 4 (deferrals available)</li>
    <li><strong>Academic Integrity:</strong> Individual assessments; AI tools permitted if you understand submitted work</li>
  </ul>

  <p><em>Last Updated: January 2025</em></p>
</article>
